<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Mududu">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Mududu">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mududu">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Mududu</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mududu</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/18/neural-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/18/neural-network/" class="post-title-link" itemprop="url">neural-network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-18 14:30:26 / 修改时间：14:38:05" itemprop="dateCreated datePublished" datetime="2022-10-18T14:30:26+08:00">2022-10-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="1-神经网络初探"><a href="#1-神经网络初探" class="headerlink" title="1 神经网络初探"></a>1 神经网络初探</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用numpy生成随机数模拟输入神经网络的数据</span></span><br><span class="line">X = np.random.randint(<span class="number">0</span>, <span class="number">5</span>, size=(<span class="number">500</span>, <span class="number">500</span>))  <span class="comment"># X:数据集的样本特征,共有有效数据500条,每条数据500个特征</span></span><br><span class="line">Y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">1</span>, <span class="number">500</span>))  <span class="comment"># y:数据集的样本标签,500条有效数据，每条对应一个标签，共有500个标签</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="comment"># 本函数为权重参数初始化函数</span></span><br><span class="line"><span class="comment"># 自行决定初始化参数的方式（例如：全零初始化，随机初始化等）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_parameters</span>(<span class="params">in_, out_</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    in_ : 输入层神经元个数</span></span><br><span class="line"><span class="string">    out_: 输出层神经元个数</span></span><br><span class="line"><span class="string">    w: 权重参数weights</span></span><br><span class="line"><span class="string">    b: 偏置参数bias</span></span><br><span class="line"><span class="string">    设计思路:w,b的规模应当满足前向传播计算中矩阵乘法的计算要求</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    w = np.random.randn(out_,in_)*<span class="number">0.01</span></span><br><span class="line">    b = np.random.randn(out_,<span class="number">1</span>)*<span class="number">0.01</span></span><br><span class="line">    <span class="keyword">assert</span> (w.shape == (out_,in_))</span><br><span class="line">    <span class="keyword">assert</span> (b.shape == (out_,<span class="number">1</span>))</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&#x27;w&#x27;</span>: w,</span><br><span class="line">        <span class="string">&#x27;b&#x27;</span>: b</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters  <span class="comment"># 该函数返回初始化好的权重参数字典</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数为前向传播输出预测值</span></span><br><span class="line"><span class="comment"># 本函数实现对字典的更新即可，无需返回值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feed_forward</span>(<span class="params">parameters, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    parameters: 参数字典,即初始化的权重参数</span></span><br><span class="line"><span class="string">    X: 输入值</span></span><br><span class="line"><span class="string">    y_pred: 预测值</span></span><br><span class="line"><span class="string">    设计思路:利用参数字典中的w,b和输入值X，先进性线性变换，再进行激活，完成前向传播。将得到的y_pred添加保存至参数字典中。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w1 = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    z1 = np.dot(w1,X) + b1</span><br><span class="line">    A1 = sigmoid(z1)</span><br><span class="line">    cache = &#123;</span><br><span class="line">        <span class="string">&quot;A1&quot;</span>: A1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (A1,cache)</span><br><span class="line"><span class="comment"># 本函数为反向传播求梯度</span></span><br><span class="line"><span class="comment"># 通过链式法则求出梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">parameters, X, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    parameters: 参数字典</span></span><br><span class="line"><span class="string">    X: 输入数据</span></span><br><span class="line"><span class="string">    y: 输出数据</span></span><br><span class="line"><span class="string">    dw: 权重参数的梯度</span></span><br><span class="line"><span class="string">    db: 偏置参数的梯度</span></span><br><span class="line"><span class="string">    设计思路:通过链式法则，得到w,b对损失函数的偏导数，得出dw,db。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w1 = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    A1 = cache[<span class="string">&quot;A1&quot;</span>]</span><br><span class="line">    dw = (<span class="number">1</span>/<span class="number">1</span>)*np.dot(X,(A1-Y).T)<span class="comment">#(500,1)</span></span><br><span class="line">    db = (<span class="number">1</span>/<span class="number">1</span>)*np.<span class="built_in">sum</span>(A1-Y)</span><br><span class="line">    <span class="comment">#print(db.shape)</span></span><br><span class="line">    <span class="comment">#print(dw.shape,w1.shape,A1.shape,X)</span></span><br><span class="line">    <span class="comment">#assert (dw.shape == w1.shape)</span></span><br><span class="line">    grad = &#123;</span><br><span class="line">        <span class="string">&#x27;dw&#x27;</span>: dw,</span><br><span class="line">        <span class="string">&#x27;db&#x27;</span>: db</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> grad  <span class="comment"># 该函数返回通过反向传播求出的梯度字典</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数为使用梯度下降法更新权重参数</span></span><br><span class="line"><span class="comment"># 所用梯度为经反向传播求得的梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">grad, parameters, lr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    grad: 通过反向传播求出的梯度,保存了当前的dw,db</span></span><br><span class="line"><span class="string">    parameters: 参数字典,保存了当前的w,b</span></span><br><span class="line"><span class="string">    lr: 学习率,影响梯度下降的速度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    b = parameters[<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    dw = grad[<span class="string">&quot;dw&quot;</span>]</span><br><span class="line">    db = grad[<span class="string">&quot;db&quot;</span>]</span><br><span class="line">    w = w - lr*dw</span><br><span class="line">    b = b - lr*db</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&quot;w&quot;</span>: w,</span><br><span class="line">        <span class="string">&quot;b&quot;</span>: b</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters  <span class="comment"># 该函数返回更新后的参数字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parameters = initial_parameters(<span class="number">500</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line">    A1,cache = feed_forward(parameters,X)</span><br><span class="line">    grad = gradient(parameters,X,Y)</span><br><span class="line">    parameters = update(grad, parameters , <span class="number">0.01</span>)</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第一题较为基础，具体思考过程放至第二题展示</p>
<h2 id="2-搭建属于你的神经网络"><a href="#2-搭建属于你的神经网络" class="headerlink" title="2 搭建属于你的神经网络"></a>2 搭建属于你的神经网络</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol>
<li>代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = pd.read_csv(<span class="string">&quot;UCI Heart Disease Dataset.csv&quot;</span>)</span><br><span class="line">    y = x[<span class="string">&quot;target&quot;</span>][:,np.newaxis]</span><br><span class="line">    <span class="keyword">del</span> x[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">    x = np.array(x)</span><br><span class="line">    y = np.array(y)</span><br><span class="line">    y.astype(np.int64)</span><br><span class="line">    x = x.T</span><br><span class="line">    y =y.T</span><br><span class="line">    </span><br></pre></td></tr></table></figure></li>
<li>解释：<ol>
<li>转为矩阵更方便各类处理(reshape,切片….)</li>
</ol>
</li>
</ol>
<h3 id="初始化权重参数"><a href="#初始化权重参数" class="headerlink" title="初始化权重参数"></a>初始化权重参数</h3><ol>
<li>代码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X , Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">     X - 输入数据集,维度为（输入的数量，训练/测试的数量）</span></span><br><span class="line"><span class="string">     Y - 标签，维度为（输出的数量，训练/测试数量）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">     n_x - 输入层的数量</span></span><br><span class="line"><span class="string">     n_h - 隐藏层的数量</span></span><br><span class="line"><span class="string">     n_y - 输出层的数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment">#输入层</span></span><br><span class="line">    n_h = <span class="number">30</span> <span class="comment">#，隐藏层个数，硬编码为30</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment">#输出层</span></span><br><span class="line">    <span class="keyword">return</span> (n_x,n_h,n_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_parameters</span>(<span class="params">in_,hid_,out_</span>):</span><br><span class="line">       np.random.seed(<span class="number">2</span>)</span><br><span class="line">    w1 = np.random.randn(hid_,in_)*np.sqrt(<span class="number">1</span>/<span class="number">13</span>)</span><br><span class="line">    b1 = np.zeros(shape=(hid_,<span class="number">1</span>))</span><br><span class="line">    w2 = np.random.randn(out_,hid_)*np.sqrt(<span class="number">1</span>/hid_)</span><br><span class="line">    b2 = np.zeros(shape=(out_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#适用断言确保数据维度初始化正确</span></span><br><span class="line">    <span class="keyword">assert</span> (w1.shape == (hid_,in_))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (hid_,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (w2.shape == (out_,hid_))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (out_,<span class="number">1</span>))</span><br><span class="line">    parameters =&#123;</span><br><span class="line">        <span class="string">&quot;w1&quot;</span>: w1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;w2&quot;</span>: w2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li>
<li>为何w随机，b全0：<ol>
<li>w、b均全0初始化缺点：<ol>
<li>若用全0初始化，则会使wx始终为0，使后续的参数迭代与初始数据无关，导致学习失败。</li>
<li><strong>权重的对称性</strong>：如每个隐层的神经元的w都相等，正向传播的时候所有神经元的输出都一致。在反向传播的时候各隐藏层更新也相同。导致多个隐藏神经元的作用如同1个神经元。</li>
</ol>
</li>
<li>w全0，b随机初始化缺点：<ol>
<li>第一次batch的时候由于w为0，使得除了最后一层的w之外的w得不到更新。第二次batch时才可开始更新。</li>
<li>存在更新较慢、梯度消失、梯度爆炸等问题</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h3><ol>
<li>参数：<ol>
<li>定义：就是模型可以根据数据可以自动学习出的变量</li>
<li>本题：各层神经元的权重（w）、偏置（b）</li>
</ol>
</li>
<li>超参数：<ol>
<li>定义：就是用来确定模型的一些参数</li>
<li>本题：学习速率（lr）、迭代次数（iretation_num）、神经网络的层数、每层神经元的个数等</li>
</ol>
</li>
</ol>
<h3 id="激活函数与损失函数"><a href="#激活函数与损失函数" class="headerlink" title="激活函数与损失函数"></a>激活函数与损失函数</h3><ol>
<li><strong>激活函数</strong>：<ol>
<li>作用：<ol>
<li>部分较极端神经网络经过多层叠加会走向极限，影响最终预测。可用激活函数将其限制于一定范围内。</li>
<li>线性回归的表达能力太弱了，对于y&#x3D;ax+b无论叠加多少层最后仍然是线性的，增加网络的深度根本没有意义。激活函数可令神经网络拥有非线性的表达能力。</li>
</ol>
</li>
<li>本题：<ol>
<li>sigmoid函数：二分类问题，最后一层用sigmoid可使最终结果位于（0，1）以表达判断概率。<ol>
<li>缺点：<ol>
<li>梯度消失可能性大</li>
<li>在非最后一层使用会使下一层的输入全为正，使得反向传播更新参数时w全往同一方向更新（捆绑效果）</li>
<li>幂运算较为耗时，增加训练时间</li>
</ol>
</li>
</ol>
</li>
<li>tanh函数<ol>
<li>解决了Sigmoid函数的不是zero-centered输出问题</li>
</ol>
</li>
<li>Relu函数：<ol>
<li>优点<ol>
<li>计算速度快，收敛速度快</li>
<li>正区间中解决梯度消失问题</li>
</ol>
</li>
<li>缺点：<ol>
<li>非零中心</li>
<li>极端情况下某些神经元不会被激活。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">1</span> + np.tanh(<span class="number">0.5</span> * x))</span><br><span class="line"><span class="comment">#防止sigmoid溢出的安全写法</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><strong>损失函数</strong>：<ol>
<li><strong>交叉熵损失函数（二分类）</strong>：<ol>
<li><img src="/2.png"></li>
<li>多分类问题：<br> <img src="/3.png"></li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ol>
<li>工具使用：matplotlib</li>
<li>代码实现：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    plt.figure(<span class="number">1</span>)</span><br><span class="line">    plt.scatter(i,cost)</span><br><span class="line">    <span class="comment">#i为循环次数，cost为计算出的交叉熵损失，此处绘制散点图。</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li>结果展示：<br><img src="/4.png"></li>
</ol>
<h3 id="准确率计算"><a href="#准确率计算" class="headerlink" title="准确率计算"></a>准确率计算</h3><ol>
<li>代码实现：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cache,a2 = feed_foward(x,parameters)</span><br><span class="line">predictions = a2</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;准确率: %d&#x27;</span> % <span class="built_in">float</span>((np.dot(y, predictions.T) + \</span><br><span class="line">                          np.dot(<span class="number">1</span> - y, <span class="number">1</span> - predictions.T)) / <span class="built_in">float</span>(y.size) * <span class="number">100</span>) + <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li>原理解释： accuracy &#x3D; (TP + TN) &#x2F; (TP + FP + TN + FN)</li>
<li>结果：在30个神经元，学习率0.001，循环1500次下准确率仅有54%。在8个神经元，lr&#x3D;0.001，循环100w次准确率有90%以上。本题若想提升准确率还需扩大数据量或提高隐层个数。</li>
</ol>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">warnings.filterwarnings((<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">1</span> + np.tanh(<span class="number">0.5</span> * x))</span><br><span class="line">    <span class="comment">#sigmoid平替</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X , Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">     X - 输入数据集,维度为（输入的数量，训练/测试的数量）</span></span><br><span class="line"><span class="string">     Y - 标签，维度为（输出的数量，训练/测试数量）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">     n_x - 输入层的数量</span></span><br><span class="line"><span class="string">     n_h - 隐藏层的数量</span></span><br><span class="line"><span class="string">     n_y - 输出层的数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment">#输入层</span></span><br><span class="line">    n_h = <span class="number">30</span> <span class="comment">#，隐藏层，硬编码为30</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment">#输出层</span></span><br><span class="line">    <span class="keyword">return</span> (n_x,n_h,n_y)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># 计算每行的最大值</span></span><br><span class="line">    row_max = np.<span class="built_in">max</span>(x)</span><br><span class="line">    <span class="comment"># 每行元素都需要减去对应的最大值，否则求exp(x)会溢出，导致inf情况</span></span><br><span class="line">    x = x - row_max</span><br><span class="line">    <span class="comment"># 计算e的指数次幂</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line">    x_sum = np.<span class="built_in">sum</span>(x_exp)</span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">    <span class="comment">#用于多分类问题，保证p（总）=1.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_parameters</span>(<span class="params">in_,hid_,out_</span>):</span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    w1 = np.random.randn(hid_,in_)*np.sqrt(<span class="number">1</span>/<span class="number">13</span>)</span><br><span class="line">    b1 = np.zeros(shape=(hid_,<span class="number">1</span>))</span><br><span class="line">    w2 = np.random.randn(out_,hid_)*np.sqrt(<span class="number">1</span>/hid_)</span><br><span class="line">    b2 = np.zeros(shape=(out_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># assert (w1.shape == (hid_,in_))</span></span><br><span class="line">    <span class="comment"># assert (b1.shape == (hid_,1))</span></span><br><span class="line">    <span class="comment"># assert (w2.shape == (out_,hid_))</span></span><br><span class="line">    <span class="comment"># assert (b2.shape == (hid_,1))</span></span><br><span class="line">    <span class="comment">#可使用断言保证数据维度初始化正确</span></span><br><span class="line">    parameters =&#123;</span><br><span class="line">        <span class="string">&quot;w1&quot;</span>: w1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;w2&quot;</span>: w2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feed_foward</span>(<span class="params">x,parameters</span>):</span><br><span class="line">    w1 = parameters[<span class="string">&quot;w1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    w2 = parameters[<span class="string">&quot;w2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    z1 = np.dot(w1,x)+b1</span><br><span class="line">    a1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(w2,a1)+b2</span><br><span class="line">    a2 = logistic_function(z2)</span><br><span class="line">    <span class="comment">#a2 = sigmoid(z2)</span></span><br><span class="line">    cache = &#123;</span><br><span class="line">        <span class="string">&quot;z1&quot;</span>:z1,</span><br><span class="line">        <span class="string">&quot;a1&quot;</span>:a1,</span><br><span class="line">        <span class="string">&quot;z2&quot;</span>:z2,</span><br><span class="line">        <span class="string">&quot;a2&quot;</span>:a2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (cache,a2)</span><br><span class="line">    <span class="comment">#cache用于储存已计算的结果，加快反向传播速度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prediction</span>(<span class="params">x,parameters</span>):</span><br><span class="line">    cache,a2 = feed_foward(x,parameters)</span><br><span class="line">    predictions = np.<span class="built_in">round</span>(a2)</span><br><span class="line">    <span class="comment">#取整，以0/1表示分类结果</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">a2,y</span>):</span><br><span class="line">    m = y.shape[<span class="number">1</span>]</span><br><span class="line">    logprobs = np.multiply(np.log(a2),y)+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-a2))</span><br><span class="line">    <span class="comment">#计算交叉熵损失</span></span><br><span class="line">    cost = - np.<span class="built_in">sum</span>(logprobs) / m</span><br><span class="line">    <span class="comment">#求平均值</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">parameters,cache,x,y</span>):</span><br><span class="line">    m = x.shape[<span class="number">1</span>]</span><br><span class="line">    w1 = parameters[<span class="string">&quot;w1&quot;</span>]</span><br><span class="line">    w2 = parameters[<span class="string">&quot;w2&quot;</span>]</span><br><span class="line">    a1 = cache[<span class="string">&quot;a1&quot;</span>]</span><br><span class="line">    a2 = cache[<span class="string">&quot;a2&quot;</span>]</span><br><span class="line">    dz2 = a2-y</span><br><span class="line">    dw2 = (<span class="number">1</span>/m)*np.dot(dz2,a1.T)</span><br><span class="line">    db2 = (<span class="number">1</span>/m)*np.<span class="built_in">sum</span>(dz2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    dz1 = np.multiply(np.dot(w2.T,dz2),<span class="number">1</span>-np.power(a1,<span class="number">2</span>))</span><br><span class="line">    dw1 = (<span class="number">1</span>/m)*np.dot(dz1,x.T)</span><br><span class="line">    db1 = (<span class="number">1</span>/m)*np.<span class="built_in">sum</span>(dz1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#向量化计算反向传播，返回w、b等偏导数的矩阵</span></span><br><span class="line">    grads = &#123;</span><br><span class="line">        <span class="string">&quot;dw2&quot;</span>:dw2,</span><br><span class="line">        <span class="string">&quot;db2&quot;</span>:db2,</span><br><span class="line">        <span class="string">&quot;dw1&quot;</span>:dw1,</span><br><span class="line">        <span class="string">&quot;db1&quot;</span>:db1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters,grads,lr</span>):</span><br><span class="line">    w1 = parameters[<span class="string">&quot;w1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    w2 = parameters[<span class="string">&quot;w2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    dw1,dw2 = grads[<span class="string">&quot;dw1&quot;</span>],grads[<span class="string">&quot;dw2&quot;</span>]</span><br><span class="line">    db1,db2 = grads[<span class="string">&quot;db1&quot;</span>],grads[<span class="string">&quot;db2&quot;</span>]</span><br><span class="line">    w1 = w1 - lr * dw1</span><br><span class="line">    b1 = b1 - lr * db1</span><br><span class="line">    w2 = w2 - lr * dw2</span><br><span class="line">    b2 = b2 - lr * db2</span><br><span class="line">    <span class="comment">#更新参数</span></span><br><span class="line">    <span class="comment">#学习率过大：跳过最优解，函数难以收敛</span></span><br><span class="line">    <span class="comment">#学习率过小：网络收敛非常缓慢，会增大找到最优值的时间。很可能会进入局部极值点就收敛，没有真正找到的最优解。</span></span><br><span class="line">    parameters =&#123;</span><br><span class="line">        <span class="string">&quot;w1&quot;</span>: w1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;w2&quot;</span>: w2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    x = pd.read_csv(<span class="string">&quot;UCI Heart Disease Dataset.csv&quot;</span>)</span><br><span class="line">    y = x[<span class="string">&quot;target&quot;</span>][:,np.newaxis]</span><br><span class="line">    <span class="keyword">del</span> x[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">    x = np.array(x)</span><br><span class="line">    y = np.array(y)</span><br><span class="line">    y.astype(np.int64)</span><br><span class="line">    x = x.T</span><br><span class="line">    y =y.T</span><br><span class="line">    n_x, n_h, n_y = layer_sizes(x,y)</span><br><span class="line">    parameters = initial_parameters(n_x,n_h,n_y)</span><br><span class="line">    num_iterations =<span class="number">1500</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (num_iterations):</span><br><span class="line">        cache, a2 = feed_foward(x, parameters)</span><br><span class="line">        cost = compute_cost(a2,y)</span><br><span class="line">        grads = gradient(parameters, cache, x, y)</span><br><span class="line">        parameters = update_parameters(parameters,grads,<span class="number">0.001</span>)</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(cost)</span><br><span class="line">        plt.figure(<span class="number">1</span>)</span><br><span class="line">        plt.scatter(i,cost)</span><br><span class="line">    plt.show()</span><br><span class="line">    cache,a2 = feed_foward(x,parameters)</span><br><span class="line">    predictions = a2</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;准确率: %d&#x27;</span> % <span class="built_in">float</span>((np.dot(y, predictions.T) + \</span><br><span class="line">                              np.dot(<span class="number">1</span> - y, <span class="number">1</span> - predictions.T)) / <span class="built_in">float</span>(y.size) * <span class="number">100</span>) + <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/18/random-forest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/18/random-forest/" class="post-title-link" itemprop="url">random-forest</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-18 14:29:44 / 修改时间：14:38:10" itemprop="dateCreated datePublished" datetime="2022-10-18T14:29:44+08:00">2022-10-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="别动！放下那蘑菇！"><a href="#别动！放下那蘑菇！" class="headerlink" title="别动！放下那蘑菇！"></a>别动！放下那蘑菇！</h2><h3 id="解题流程"><a href="#解题流程" class="headerlink" title="解题流程"></a>解题流程</h3><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>1.<strong>缺失值处理</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#统计缺失值比例</span></span><br><span class="line">mis_mushroom = mushroom.isnull().<span class="built_in">sum</span>() / <span class="built_in">len</span>(mushroom)</span><br><span class="line">mis_percent = mis_mushroom.sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(mis_percent)</span><br></pre></td></tr></table></figure>
<p>   <img src="/1.png"></p>
<ol>
<li><strong>删</strong>：通过统计缺失率，将大部分缺失的数据直接整列删去，留下缺失率较小的以待填补<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label = [<span class="string">&#x27;veil-type&#x27;</span>, <span class="string">&#x27;spore-print-color&#x27;</span>, <span class="string">&#x27;veil-color&#x27;</span>, <span class="string">&#x27;stem-root&#x27;</span>]</span><br><span class="line">mushroom = mushroom.drop(label, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>补</strong>：<ol>
<li>先通过Counter函数统计各项出现次数，然后再以众数填补（此处若使用随机森林填补效果更佳）。</li>
<li>fillna填充<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mushroom[<span class="string">&#x27;stem-surface&#x27;</span>] = mushroom[<span class="string">&#x27;stem-surface&#x27;</span>].fillna(<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">mushroom[<span class="string">&#x27;gill-spacing&#x27;</span>] = mushroom[<span class="string">&#x27;gill-spacing&#x27;</span>].fillna(<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">mushroom[<span class="string">&#x27;cap-surface&#x27;</span>] = mushroom[<span class="string">&#x27;cap-surface&#x27;</span>].fillna(<span class="string">&#x27;t&#x27;</span>)</span><br><span class="line">mushroom[<span class="string">&#x27;gill-attachment&#x27;</span>] = mushroom[<span class="string">&#x27;gill-attachment&#x27;</span>].fillna(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">mushroom[<span class="string">&#x27;ring-type&#x27;</span>] = mushroom[<span class="string">&#x27;ring-type&#x27;</span>].fillna(<span class="string">&#x27;f&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><strong>字符型数据转换</strong><ol>
<li>方法一：LabelEncoder： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder=LabelEncoder()</span><br><span class="line"><span class="keyword">for</span> co <span class="keyword">in</span> mushroom.columns:</span><br><span class="line">    mushroom[co] = encoder.fit_transform(mushroom[co])</span><br></pre></td></tr></table></figure></li>
<li>方法二：哑变量 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mushroom = pd.get_dummies(mushroom)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><strong>可视化</strong><ol>
<li>各数据相关性：通过热图显示，“gill-spacing”与“stem-surface”和“gill-color相关性较高”。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   sns.heatmap(mushroom.corr(),annot=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/3.png"></li>
<li>可视化分析</li>
<li><img src="/4.png">展示每种菌盖颜色的出现次数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   cap = mushroom[<span class="string">&quot;cap-color&quot;</span>].value_counts().reset_index()</span><br><span class="line">cap.columns = [<span class="string">&quot;color&quot;</span>, <span class="string">&quot;number&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(cap)</span><br><span class="line">fig = px.bar(cap,x= <span class="string">&quot;color&quot;</span>,y=<span class="string">&quot;number&quot;</span>,color=<span class="string">&quot;number&quot;</span></span><br><span class="line">             ,text=<span class="string">&quot;number&quot;</span>,color_continuous_scale=<span class="string">&quot;rainbow&quot;</span>)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure></li>
<li><img src="/5.png">统计无毒与有毒的蘑菇的颜色分布<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   cap_class = mushroom.groupby([<span class="string">&quot;class&quot;</span>,<span class="string">&quot;cap-color&quot;</span>]).size().reset_index()</span><br><span class="line">cap_class.columns = [<span class="string">&quot;class&quot;</span>,<span class="string">&quot;color&quot;</span>,<span class="string">&quot;number&quot;</span>]</span><br><span class="line">fig = px.bar(cap_class,x= <span class="string">&quot;color&quot;</span>,y=<span class="string">&quot;number&quot;</span>,color=<span class="string">&quot;class&quot;</span></span><br><span class="line">             ,text=<span class="string">&quot;number&quot;</span>,barmode=<span class="string">&quot;group&quot;</span>)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure></li>
<li><strong>分析结果</strong>：n颜色的蘑菇最多，而仅有b颜色的蘑菇无毒率较高，其余蘑菇均有约50%以上的有毒率。<br><strong>同理可以对于蘑菇的气味等数据进行可视化分析</strong></li>
</ol>
</li>
<li><strong>标准化</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler = StandardScaler()</span><br><span class="line">mushroom = scaler.fit_transform(mushroom)</span><br></pre></td></tr></table></figure></li>
<li><strong>PCA</strong><ol>
<li><img src="/7.png"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> plt.style.context(<span class="string">&quot;dark_background&quot;</span>):  <span class="comment"># 背景</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))  <span class="comment"># 大小</span></span><br><span class="line"></span><br><span class="line">  plt.bar(<span class="built_in">range</span>(<span class="number">16</span>),  <span class="comment"># 主成分个数</span></span><br><span class="line">          explained_variance,  <span class="comment"># 方差值</span></span><br><span class="line">          alpha=<span class="number">0.5</span>,  <span class="comment"># 透明度</span></span><br><span class="line">          align=<span class="string">&quot;center&quot;</span>,</span><br><span class="line">          label=<span class="string">&quot;individual explained variance&quot;</span>  <span class="comment"># 标签</span></span><br><span class="line">          )</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;Explained variance ratio&#x27;</span>)  <span class="comment"># 轴名称和图例</span></span><br><span class="line">  plt.xlabel(<span class="string">&#x27;Principal components&#x27;</span>)</span><br><span class="line">  plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure>
由图像得，前十四位方差均较大，第十五位方差较小，与其余属性相关性太强，可直接删除以避免造成数据冗余。</li>
</ol>
</li>
</ol>
<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><ol>
<li><strong>网格搜索+训练</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">estimator = RandomForestClassifier()</span><br><span class="line"> param_dict = &#123;<span class="string">&quot;n_estimators&quot;</span>:[ <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>], <span class="string">&quot;max_depth&quot;</span>:[<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>]&#125;</span><br><span class="line"> estimator = GridSearchCV(estimator,param_grid=param_dict,cv=<span class="number">3</span>)</span><br><span class="line"> estimator.fit(x_train, y_train)</span><br></pre></td></tr></table></figure></li>
<li><strong>模型评估</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">score = estimator.score(x_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率为&quot;</span>,score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最佳参数&quot;</span>,estimator.best_params_)</span><br></pre></td></tr></table></figure>
<img src="/6.png"></li>
</ol>
<h3 id="模型原理（随机森林）"><a href="#模型原理（随机森林）" class="headerlink" title="模型原理（随机森林）"></a>模型原理（随机森林）</h3><ol>
<li><strong>概念</strong>：<ol>
<li>随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。它的工作原理是生成多个分类器&#x2F;模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</li>
<li>理解：“聪明树的想法总是差不多”，但笨树总是各有各的笨法”，所以选取众数的方法可以达到较好准确率（将弱分类器聚合为强分类器）</li>
</ol>
</li>
<li><strong>实现流程</strong>：<ol>
<li>用N来表示训练用例（样本）的个数，M表示特征数目。<ol>
<li>一次随机选出一个样本，重复N次， （有可能出现重复的样本）</li>
<li>随机去选出m个特征, m &lt;&lt;M，建立决策树</li>
</ol>
</li>
<li>采取bootstrap抽样<ol>
<li>保证每棵树不完全一样</li>
<li>保证每棵树训练样本相同</li>
</ol>
</li>
</ol>
</li>
<li>**API(常用)**：<ol>
<li>n_estimators ： 整数，可选(默认&#x3D; 100),森林里的树木数量。</li>
<li>max_depth ： 整数或无，可选（默认&#x3D;无)树的最大深度。</li>
</ol>
</li>
<li><strong>完整代码</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span>  sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> plotly_express <span class="keyword">as</span> px</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings((<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_poison</span>():</span><br><span class="line">    <span class="comment">#获取数据</span></span><br><span class="line">    mushroom = pd.read_csv(<span class="string">&quot;data.csv&quot;</span>, sep=<span class="string">&#x27;;&#x27;</span>)</span><br><span class="line">    <span class="comment">#统计缺失值比例</span></span><br><span class="line">    mis_mushroom = mushroom.isnull().<span class="built_in">sum</span>() / <span class="built_in">len</span>(mushroom)</span><br><span class="line">    mis_percent = mis_mushroom.sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(mis_percent)</span><br><span class="line">    <span class="comment">#数据处理：缺失值</span></span><br><span class="line">    label = [<span class="string">&#x27;veil-type&#x27;</span>, <span class="string">&#x27;spore-print-color&#x27;</span>, <span class="string">&#x27;veil-color&#x27;</span>, <span class="string">&#x27;stem-root&#x27;</span>]</span><br><span class="line">    mushroom = mushroom.drop(label, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#label2 = [&#x27;stem-surface&#x27;,&#x27;gill-spacing&#x27;,&#x27;cap-surface&#x27;,&#x27;gill-attachment&#x27;,&#x27;ring-type&#x27;]</span></span><br><span class="line">    mushroom[<span class="string">&#x27;stem-surface&#x27;</span>] = mushroom[<span class="string">&#x27;stem-surface&#x27;</span>].fillna(<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    mushroom[<span class="string">&#x27;gill-spacing&#x27;</span>] = mushroom[<span class="string">&#x27;gill-spacing&#x27;</span>].fillna(<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">    mushroom[<span class="string">&#x27;cap-surface&#x27;</span>] = mushroom[<span class="string">&#x27;cap-surface&#x27;</span>].fillna(<span class="string">&#x27;t&#x27;</span>)</span><br><span class="line">    mushroom[<span class="string">&#x27;gill-attachment&#x27;</span>] = mushroom[<span class="string">&#x27;gill-attachment&#x27;</span>].fillna(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    mushroom[<span class="string">&#x27;ring-type&#x27;</span>] = mushroom[<span class="string">&#x27;ring-type&#x27;</span>].fillna(<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line">    <span class="comment">#可视化/相关性</span></span><br><span class="line">    cap = mushroom[<span class="string">&quot;cap-color&quot;</span>].value_counts().reset_index()</span><br><span class="line">    cap.columns = [<span class="string">&quot;color&quot;</span>, <span class="string">&quot;number&quot;</span>]</span><br><span class="line">    fig = px.bar(cap,x= <span class="string">&quot;color&quot;</span>,y=<span class="string">&quot;number&quot;</span>,color=<span class="string">&quot;number&quot;</span></span><br><span class="line">                 ,text=<span class="string">&quot;number&quot;</span>,color_continuous_scale=<span class="string">&quot;rainbow&quot;</span>)</span><br><span class="line">    fig.show()</span><br><span class="line">    cap_class = mushroom.groupby([<span class="string">&quot;class&quot;</span>,<span class="string">&quot;cap-color&quot;</span>]).size().reset_index()</span><br><span class="line">    cap_class.columns = [<span class="string">&quot;class&quot;</span>,<span class="string">&quot;color&quot;</span>,<span class="string">&quot;number&quot;</span>]</span><br><span class="line">    fig = px.bar(cap_class,x= <span class="string">&quot;color&quot;</span>,y=<span class="string">&quot;number&quot;</span>,color=<span class="string">&quot;class&quot;</span></span><br><span class="line">                 ,text=<span class="string">&quot;number&quot;</span>,barmode=<span class="string">&quot;group&quot;</span>)</span><br><span class="line">    fig.show()</span><br><span class="line">    <span class="comment">#特征工程</span></span><br><span class="line">    <span class="comment">#特征转换</span></span><br><span class="line">    encoder=LabelEncoder()</span><br><span class="line">    <span class="keyword">for</span> co <span class="keyword">in</span> mushroom.columns:</span><br><span class="line">        mushroom[co] = encoder.fit_transform(mushroom[co])</span><br><span class="line">    <span class="comment">#分离特征与标签</span></span><br><span class="line">    target = mushroom.iloc[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">del</span> mushroom[<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">    <span class="comment">#热图无法处理字符串数据</span></span><br><span class="line">    <span class="comment"># sns.heatmap(mushroom.corr(),annot=True)</span></span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line">    <span class="comment">#数据标准化</span></span><br><span class="line">    scaler = StandardScaler()</span><br><span class="line">    mushroom = scaler.fit_transform(mushroom)</span><br><span class="line">    <span class="comment">#主成分PCA</span></span><br><span class="line">    pca =PCA()</span><br><span class="line">    pca.fit_transform(mushroom)</span><br><span class="line">    convariance = pca.get_covariance()</span><br><span class="line">    explained_variance = pca.explained_variance_</span><br><span class="line">    <span class="keyword">with</span> plt.style.context(<span class="string">&quot;dark_background&quot;</span>):  <span class="comment"># 背景</span></span><br><span class="line">        plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))  <span class="comment"># 大小</span></span><br><span class="line"></span><br><span class="line">        plt.bar(<span class="built_in">range</span>(<span class="number">16</span>),  <span class="comment"># 主成分个数</span></span><br><span class="line">                explained_variance,  <span class="comment"># 方差值</span></span><br><span class="line">                alpha=<span class="number">0.5</span>,  <span class="comment"># 透明度</span></span><br><span class="line">                align=<span class="string">&quot;center&quot;</span>,</span><br><span class="line">                label=<span class="string">&quot;individual explained variance&quot;</span>  <span class="comment"># 标签</span></span><br><span class="line">                )</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Explained variance ratio&#x27;</span>)  <span class="comment"># 轴名称和图例</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Principal components&#x27;</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="comment">#绘图展示主成分关系</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(mushroom,target)</span><br><span class="line">    <span class="comment">#网格搜索</span></span><br><span class="line">    estimator = RandomForestClassifier()</span><br><span class="line">    param_dict = &#123;<span class="string">&quot;n_estimators&quot;</span>:[ <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>], <span class="string">&quot;max_depth&quot;</span>:[<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>]&#125;</span><br><span class="line">    estimator = GridSearchCV(estimator,param_grid=param_dict,cv=<span class="number">3</span>)</span><br><span class="line">    <span class="comment">#训练模型</span></span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    <span class="comment">#模型评估</span></span><br><span class="line">    score = estimator.score(x_test,y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;准确率为&quot;</span>,score)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;最佳参数&quot;</span>,estimator.best_params_)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    check_poison()</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/18/decide-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/18/decide-tree/" class="post-title-link" itemprop="url">决策树初探</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-18 14:29:15 / 修改时间：14:37:51" itemprop="dateCreated datePublished" datetime="2022-10-18T14:29:15+08:00">2022-10-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="决策树算法原理"><a href="#决策树算法原理" class="headerlink" title="决策树算法原理"></a>决策树算法原理</h4><ol>
<li>基本概念<ol>
<li><strong>信息熵</strong>：解决问题不确定性所需要的信息量,单位为比特。<img src="/2.png"></li>
<li><strong>信息增益</strong>：<br> eg:青年中年老年各五人，n&#x3D;3,Di&#x3D;5,D&#x3D;15<br> H(D|A)&#x3D;H(青年)+H(中年)+H(老年)<br> 特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为： g(D,A)&#x3D;H(D)-H(D|A)，<img src="/5.png"> <img src="/3.png"></li>
</ol>
</li>
<li><strong>构造过程</strong><ol>
<li>通过公式计算出某一条件下系统熵值变化（信息增益）。信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好。即可据其大小来决定优先判断哪个特征(更靠近根节点)。</li>
</ol>
</li>
<li><strong>信息增益率（C4.5）与gini系数（CART）</strong><ol>
<li>使用背景：出现如id等信息增益巨大但无法作为判断依据的数据。</li>
<li>信息增益：<ol>
<li>引入属性A的内部信息,以作为当某一特征类别过多的惩罚。<img src="/10.png"></li>
<li>本质：H(c) 是分类类别的熵， 类别越多，熵倾向于越大；同理， H(X)是变量X的Gain, 变量X的取值越多，H(X)的值也会越大， 惩罚除数越大</li>
</ol>
</li>
<li>gini系数：<ol>
<li>gini系数：<img src="/11.png"></li>
<li>gini增益：<img src="/12.png"></li>
</ol>
</li>
</ol>
</li>
<li><strong>剪枝</strong><ol>
<li>原因：防止决策树完全分类所有数据，避免出现过拟合。要避免这种情况，也就需要进行剪枝，控制模型的复杂程度</li>
<li>方法：<ol>
<li>预剪枝：限制深度（比如指定到某一具体数值后不再进行分裂）、叶子节点个数、叶子节点样本数、信息增益量等(max_depth&#x3D; )</li>
<li>后剪枝：<ol>
<li>衡量标准：Cα(T)&#x3D;C(T)+α∗∣T leaf∣，C(T)&#x3D;gini*samples</li>
<li>判断：判断分裂之后两个叶子节点之和相加与原C(T)大小，值越大代表着损失越大，也就越不好，取值的大小是取决于我们给定的α 值，α值给出的越大，模型越会控制过拟合，值较小的时候是我们希望模型取得较好的结果，过不过拟合看的不是很重要</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="解题过程"><a href="#解题过程" class="headerlink" title="解题过程"></a>解题过程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decision_iris</span>():</span><br><span class="line">    data = pd.read_csv(<span class="string">&quot;wdbc.data&quot;</span>,names=[<span class="string">&quot;id&quot;</span>,<span class="string">&quot;diagnose&quot;</span>,<span class="string">&quot;a1&quot;</span>, <span class="string">&quot;a2&quot;</span>, <span class="string">&quot;a3&quot;</span>, <span class="string">&quot;b1&quot;</span>, <span class="string">&quot;b2&quot;</span>, <span class="string">&quot;b3&quot;</span>, <span class="string">&quot;c1&quot;</span>, <span class="string">&quot;c2&quot;</span>, <span class="string">&quot;c3&quot;</span>, <span class="string">&quot;d1&quot;</span>, <span class="string">&quot;d2&quot;</span>, <span class="string">&quot;d3&quot;</span>, <span class="string">&quot;e1&quot;</span>, <span class="string">&quot;e2&quot;</span>, <span class="string">&quot;e3&quot;</span>, <span class="string">&quot;f1&quot;</span>, <span class="string">&quot;f2&quot;</span>, <span class="string">&quot;f3&quot;</span>, <span class="string">&quot;g1&quot;</span>, <span class="string">&quot;g2&quot;</span>, <span class="string">&quot;g3&quot;</span>, <span class="string">&quot;h1&quot;</span>, <span class="string">&quot;h2&quot;</span>, <span class="string">&quot;h3&quot;</span>, <span class="string">&quot;i1&quot;</span>, <span class="string">&quot;i2&quot;</span>, <span class="string">&quot;i3&quot;</span>, <span class="string">&quot;j1&quot;</span>, <span class="string">&quot;j2&quot;</span>, <span class="string">&quot;j3&quot;</span>])<span class="comment">#读取数据集</span></span><br><span class="line">    feature_names = [<span class="string">&quot;id&quot;</span>,<span class="string">&quot;a1&quot;</span>, <span class="string">&quot;a2&quot;</span>, <span class="string">&quot;a3&quot;</span>, <span class="string">&quot;b1&quot;</span>, <span class="string">&quot;b2&quot;</span>, <span class="string">&quot;b3&quot;</span>, <span class="string">&quot;c1&quot;</span>, <span class="string">&quot;c2&quot;</span>, <span class="string">&quot;c3&quot;</span>, <span class="string">&quot;d1&quot;</span>, <span class="string">&quot;d2&quot;</span>, <span class="string">&quot;d3&quot;</span>, <span class="string">&quot;e1&quot;</span>, <span class="string">&quot;e2&quot;</span>, <span class="string">&quot;e3&quot;</span>, <span class="string">&quot;f1&quot;</span>, <span class="string">&quot;f2&quot;</span>, <span class="string">&quot;f3&quot;</span>, <span class="string">&quot;g1&quot;</span>, <span class="string">&quot;g2&quot;</span>, <span class="string">&quot;g3&quot;</span>, <span class="string">&quot;h1&quot;</span>, <span class="string">&quot;h2&quot;</span>, <span class="string">&quot;h3&quot;</span>, <span class="string">&quot;i1&quot;</span>, <span class="string">&quot;i2&quot;</span>, <span class="string">&quot;i3&quot;</span>, <span class="string">&quot;j1&quot;</span>, <span class="string">&quot;j2&quot;</span>, <span class="string">&quot;j3&quot;</span>]</span><br><span class="line">    <span class="comment">#为数据column命名（用c++做的），并保存于feature——names</span></span><br><span class="line">    target = data.iloc[:,<span class="number">1</span>]<span class="comment">#单独引用第二行diagnose结果，作为target目标值</span></span><br><span class="line">    cancer = data.drop(<span class="string">&#x27;diagnose&#x27;</span>,axis=<span class="number">1</span>)<span class="comment">#删去第二行</span></span><br><span class="line">    x_train, x_test, y_train, y_test =train_test_split(cancer, target)<span class="comment">#数据划分</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>,random_state=<span class="number">5</span>)<span class="comment">#实例化预估器，以信息熵增益作为决策树选择标准</span></span><br><span class="line">    estimator.fit(x_train, y_train)<span class="comment">#进行训练</span></span><br><span class="line">    export_graphviz(estimator,out_file=<span class="string">&quot;cancer_tree.dot&quot;</span>,feature_names =feature_names)<span class="comment">#制图，存于指定路径</span></span><br><span class="line">    score = estimator.score(x_test, y_test)<span class="comment">#评估准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;准确率为&#x27;</span>,score)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    decision_iris()</span><br></pre></td></tr></table></figure>
<p><strong>获取列名</strong><br><img src="/8.png"><br><strong>准确率结果</strong><br><img src="/7.png"><br><strong>绘制决策树</strong><br><img src="/6.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/19/tihai/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/19/tihai/" class="post-title-link" itemprop="url">题海战术两面观</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-19 15:14:47" itemprop="dateCreated datePublished" datetime="2022-09-19T15:14:47+08:00">2022-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-18 14:38:18" itemprop="dateModified" datetime="2022-10-18T14:38:18+08:00">2022-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="训练集-amp-测试集概念理解"><a href="#训练集-amp-测试集概念理解" class="headerlink" title="训练集&amp;测试集概念理解"></a>训练集&amp;测试集概念理解</h2><ol>
<li>二者均可来自同一组数据.只不过经过一定比例的切割后,分出大部分来作为数据训练模型、进行拟合,而少部分的数据被留下以检测建立模型的拟合好坏(近似于演绎推理法).</li>
</ol>
<h2 id="过拟合-欠拟合-出现的原因"><a href="#过拟合-欠拟合-出现的原因" class="headerlink" title="过拟合(欠拟合)出现的原因"></a>过拟合(欠拟合)出现的原因</h2><ol>
<li>噪音干扰大,模型错将噪音理解为特征.</li>
<li>数据过少,使得模型过于特殊(拟合训练数据太好).</li>
<li>模型构建不合理.</li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/e387848be269">模型过于复杂</a><img src="https://upload-images.jianshu.io/upload_images/12519225-8a70f40731f0abdc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/801/format/webp" alt="1"></li>
</ol>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法:"></a>解决方法:</h4><h5 id="数据"><a href="#数据" class="headerlink" title="数据:"></a>数据:</h5><ol>
<li>获取更多数据</li>
<li>数据增强(通过对图像的处理使数据成倍增加)</li>
<li>获取额外数据进行交叉验证.</li>
</ol>
<h5 id="模型"><a href="#模型" class="headerlink" title="模型:"></a>模型:</h5><ol>
<li>降低复杂度<ol>
<li>神经网络:减少网络层数 </li>
<li>决策树:剪枝</li>
</ol>
</li>
<li>加强对于特征的选择.</li>
<li>适当加入噪声</li>
<li>正则化:向代价函数加入权值.</li>
</ol>
<p>#####欠拟合是过拟合的反状态,其出现原因与解决方法与过拟合正好相反.在此不多做赘述.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/19/population/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/19/population/" class="post-title-link" itemprop="url">第三题 - 死去的高中生物突然攻击了我</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-09-19 15:09:45 / 修改时间：15:50:32" itemprop="dateCreated datePublished" datetime="2022-09-19T15:09:45+08:00">2022-09-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learnig/" itemprop="url" rel="index"><span itemprop="name">Machine-learnig</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理:"></a>基本原理:</h3><p>（1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。</p>
<p>（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。</p>
<p>（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。<br>转载自<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27788061/article/details/88867262" title="赞美的心">CSDN阿尔法小队</a></p>
<p><strong>个人认为</strong>其原理与线性回归较为相似,都是构建代价函数以期求得minimize(j(θ)),但逻辑回归似乎更倾向于结局二项分类(概率预测)问题而非是估价等问题.由于其仅有1,0两种情况,j(θ)的图像应该仅是二维的碗型而非三维图形.如此一说,似乎梯度下降法在处理逻辑回归问题时更简单(毕竟只有单一θ,下降方向更好确定).</p>
<h3 id="人口回归"><a href="#人口回归" class="headerlink" title="人口回归"></a>人口回归</h3><pre><code>部分知识涉及高数,暂时不太理解.但以我的认识是:
增长人口增长率由于资源短缺,随人口增加而降低.故存在一点X使得增长率为0.
通过引入研究国家的历史人口增长曲线,拟合出人口固有增长率,以及最大人口数.之后再构建模型.
</code></pre>
<p><img src="https://pic3.zhimg.com/v2-4f433a6e5453b0ce5deed270aee2d21e_r.jpg" alt="引用自知乎"></p>
<h5 id="关于本篇的大量引用-我很抱歉"><a href="#关于本篇的大量引用-我很抱歉" class="headerlink" title="关于本篇的大量引用,我很抱歉!"></a>关于本篇的大量引用,我很抱歉!</h5>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/19/NBA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/19/NBA/" class="post-title-link" itemprop="url">第二题-NBA球风</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-09-19 15:09:08 / 修改时间：16:17:05" itemprop="dateCreated datePublished" datetime="2022-09-19T15:09:08+08:00">2022-09-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、概念简述-例证"><a href="#一、概念简述-例证" class="headerlink" title="一、概念简述+例证"></a>一、概念简述+例证</h2><h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习:"></a>监督学习:</h4><pre><code>通过有标签的数据建立模型对未知标签的样本进行预测(即在预处理数据时先人工分类)，其代表为&quot;分类与回归&quot;
</code></pre>
<p><strong>eg：</strong></p>
<h5 id="1-K-近邻算法"><a href="#1-K-近邻算法" class="headerlink" title="1. K-近邻算法"></a>1. K-近邻算法</h5><p><strong>原理</strong>：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。（其邻居已正确分类）<br><strong>步骤</strong>:统计距离-&gt;升序排列-&gt;确定范围(前n个点)-&gt;计算频率-&gt;返回预测分类<br>(以下三种方法暂时未能理解qwq)</p>
<h5 id="2-决策树"><a href="#2-决策树" class="headerlink" title="2.决策树"></a>2.决策树</h5><h5 id="3-朴素贝叶斯"><a href="#3-朴素贝叶斯" class="headerlink" title="3.朴素贝叶斯"></a>3.朴素贝叶斯</h5><h5 id="4-逻辑回归"><a href="#4-逻辑回归" class="headerlink" title="4.逻辑回归"></a>4.逻辑回归</h5><h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习:"></a>无监督学习:</h4><pre><code>通过对无标签数据特征进行分析，从而对无标签数据进行归类,
其代表为&quot;聚类&quot;.
</code></pre>
<p><strong>eg:</strong></p>
<h5 id="1-k-均值聚类"><a href="#1-k-均值聚类" class="headerlink" title="1.k-均值聚类:"></a>1.k-均值聚类:</h5><p><strong>原理:</strong> 主要通过不断地取离种子点最近均值的算法。<br><img src="https://img-blog.csdn.net/20180821210059884?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1B5Um9va2ll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="K-Means" title="算法示例"><br><strong>步骤</strong>:选取聚类中心-&gt;根据中心与各点间的距离再次划分聚类-&gt;迭代-&gt;确定最优聚类中心.   </p>
<h5 id="2-层次聚类-Hierarchical-Clustering"><a href="#2-层次聚类-Hierarchical-Clustering" class="headerlink" title="2.层次聚类(Hierarchical Clustering)"></a>2.层次聚类(Hierarchical Clustering)</h5><h2 id="二、差别与联系："><a href="#二、差别与联系：" class="headerlink" title="二、差别与联系："></a>二、差别与联系：</h2><h4 id="差别"><a href="#差别" class="headerlink" title="差别:"></a>差别:</h4><pre><code>最主要的差别便是训练数据有无标签
</code></pre>
<h4 id="联系"><a href="#联系" class="headerlink" title="联系:"></a>联系:</h4><pre><code>均为从已有数据中寻找规律,对未知样本进行预测.
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/19/flower-and-AI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/19/flower-and-AI/" class="post-title-link" itemprop="url">花朵与人工智能</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-19 15:08:20" itemprop="dateCreated datePublished" datetime="2022-09-19T15:08:20+08:00">2022-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-18 14:36:56" itemprop="dateModified" datetime="2022-10-18T14:36:56+08:00">2022-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、传统机器学习"><a href="#一、传统机器学习" class="headerlink" title="一、传统机器学习"></a>一、传统机器学习</h2><h3 id="1-概念："><a href="#1-概念：" class="headerlink" title="1.概念："></a>1.概念：</h3><p><strong>百度</strong>：机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能核心，是使计算机具有智能的根本途径。<br><strong>个人认为</strong>机器学习就是通过模型构建使机器具有从海量数据中寻求规律的的能力的过程。（好拗口）</p>
<h3 id="2-流程"><a href="#2-流程" class="headerlink" title="2.流程"></a>2.流程</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hzblucky1314/article/details/123320780"><img src="https://img-blog.csdnimg.cn/591720505b7b48229c5769aedb420c0c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiJ54i35bim5L2g6aOe,size_20,color_FFFFFF,t_70,g_se,x_16" alt="流程"></a><br><strong>个人总结</strong>：机器学习可总结为3步<br>（1）运用不同算法构建相应模型<br>（2）用经过预处理的数据对模型进行训练<br>（3）评估结果并调整参数以期对未知数据达到最好的预测效果</p>
<h2 id="二、深度学习"><a href="#二、深度学习" class="headerlink" title="二、深度学习"></a>二、深度学习</h2><h3 id="1-概念：-1"><a href="#1-概念：-1" class="headerlink" title="1.概念："></a>1.概念：</h3><p>深度学习是机器学习中一种基于对数据进行表征学习的方法。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</p>
<h3 id="2-流程："><a href="#2-流程：" class="headerlink" title="2.流程："></a>2.流程：</h3><p><strong>个人认为</strong>其与传统机器学习基本相同,但其在训练过程中引入了对人体神经网络的模拟.</p>
<h2 id="三、两者的区别与联系"><a href="#三、两者的区别与联系" class="headerlink" title="三、两者的区别与联系"></a>三、两者的区别与联系</h2><h2 id="1-联系"><a href="#1-联系" class="headerlink" title="1.联系"></a>1.联系</h2><p>深度学习是机器学习的子集,是实现机器学习的一种技术分支,其可使模型对于数据的理解更加深入.<br>对于数据充足、定义清晰、特征不易提取的问题DL更适用.<br>对于数据少、特征明显的问题ML更适用。<br>二者相辅相成</p>
<h2 id="2-区别"><a href="#2-区别" class="headerlink" title="2.区别"></a>2.区别</h2><h4 id="训练数据量-amp-硬件依赖"><a href="#训练数据量-amp-硬件依赖" class="headerlink" title="训练数据量&amp;硬件依赖"></a>训练数据量&amp;硬件依赖</h4><p>深度学习算法需要大量数据才能展现出更好的性能,对硬件要求也更高.(如下图)<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82703514"><img src="https://pic1.zhimg.com/80/v2-31ffa72066a76e473b40cbb345739688_1440w.jpg" alt="数据相关性" title="引用自知乎"></a><br>而机器学习由于算法以及特征工程的构建,在较小数据集下效果更好.</p>
<h4 id="特征处理方式"><a href="#特征处理方式" class="headerlink" title="特征处理方式"></a>特征处理方式</h4><p>传统机器学习的特征提取是人为处理的(有点像监督式学习?)<br>深度学习的特征提取是由深层神经网络自行提取的(无监督式学习?),其特征人类无法看懂,也称<em>黑盒子</em></p>
<h4 id="执行时间"><a href="#执行时间" class="headerlink" title="执行时间"></a>执行时间</h4><p>由于深度学习算法中有很多的参数，其需要很长时间来训练(但测试时间反而较短).而ML学习时间较短
    </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/19/auto-drive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/19/auto-drive/" class="post-title-link" itemprop="url">瞎谈机器学习与自动驾驶</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-19 14:48:04" itemprop="dateCreated datePublished" datetime="2022-09-19T14:48:04+08:00">2022-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-18 14:37:17" itemprop="dateModified" datetime="2022-10-18T14:37:17+08:00">2022-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="强化学习——自动驾驶的实现"><a href="#强化学习——自动驾驶的实现" class="headerlink" title="强化学习——自动驾驶的实现"></a>强化学习——自动驾驶的实现</h1><p>##核心流程</p>
<h2 id="环境感知定位（略）："><a href="#环境感知定位（略）：" class="headerlink" title="环境感知定位（略）："></a>环境感知定位（略）：</h2><pre><code>主要是通过传感器技术和摄像头、GPS等技术获取汽车行驶过程中的环境指标，并且将数据采集
</code></pre>
<p><strong>环境感知:</strong></p>
<ol>
<li><strong>摄像头</strong>：单目摄像头（车顶）以负责监控前方的路障.双目摄像头可以利用视距差，通过三角测距技术获取障碍物到车辆的距离.<br><img src="https://pic4.zhimg.com/80/v2-04f164324f2a46f660904882693eac6b_1440w.jpg" alt="双目"></li>
<li><strong>激光雷达</strong>:又称LiDAR，他的核心原理是通过发送一道光，这个光遇到障碍物会反弹回收，通过两者的时间差来判断距离.并可以绘制出点云图，从而实现对障碍物的形状、大小、距离的描绘.<br><img src="https://pic4.zhimg.com/80/v2-146c554547c2cfe67d138a64aec7b397_1440w.jpg" alt="雷达"></li>
<li><strong>毫米波雷达</strong>:穿透灰尘和烟的能力较强，在非常极端的天气条件下也可以正常工作.</li>
<li><strong>超声波雷达</strong>:用于倒车辅助功能.</li>
</ol>
<p><strong>定位</strong>:</p>
<ol>
<li><strong>卫星定位</strong>:通过三角定位的方案，至少3颗卫星同时跟地面车辆连接，就可以计算出准确的车辆坐标.</li>
<li><strong>差分定位</strong>:确定一个参考站，参考站跟卫星发生通信，得到当前位置的误差.再以汽车为流动站，通过差分校正量为汽车的真实定位做校正.<img src="https://pic1.zhimg.com/80/v2-3701021468f49da21bf22f5eeda4530c_1440w.jpg" alt="差分定位"></li>
<li><strong>惯性定位</strong>:依赖于汽车当前的加速度，通过积分可以获得车的下一时间段的位移，当车速较快的时候，惯性定位会是卫星定位的很好地补充.</li>
</ol>
<h2 id="决策规划："><a href="#决策规划：" class="headerlink" title="决策规划："></a>决策规划：</h2><pre><code>通过收集的数据，对车辆的下一步行为作出判断和指导
</code></pre>
<ol>
<li><strong>路径规划</strong>:涉及高精度地图领域.实质为求两点间最短路径问题.常用算法有:Dijkstra、Floyd、A*、RRT.</li>
<li><strong>行为决策</strong>:主要包含两个方面，一个是车辆自己的形行为决策，另一个是对于其它行驶车辆的行为的预测.<br>2.1 <strong>车辆自身行为决策88</strong>: 其<strong>指令集</strong>包含行驶、跟车、转弯、换道、停车等.<br><img src="https://pic2.zhimg.com/80/v2-bbbd0ac2e3952861c9e035ab19ef06dd_1440w.jpg" alt="整体流程"><br>这种自动驾驶车辆行为决策的功能可以看成是一系列概率的加成，可以看成是<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357466984">马尔科夫决策过程</a>(当前状态只与前一时刻状态有关).<br> 2.2<strong>对交通参与方的预测</strong>可以通过多种算法来实现，构建一套运动模型的方式.可以<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/8587563">高斯噪声</a>来代表交通参与者的不确定性.对于交通参与方的行为和意图的预测，可以看作是一个动态的时序过程，可以用深度学习LSTM这样的循环神经网络解决相应的问题.</li>
</ol>
<p>2.2.1 红绿灯检测和识别: 包括检测汽车周围环境中一个或多个红绿灯的位置（例如，在图像中表示）并识别其状态（红色、绿色和黄色）,交通信号灯的检测与识别方法主要分为两类：基于模型的方法(鲁棒性较差)和基于学习的方法.</p>
<p>2.2.2交通标志检测与识别:包括检测环境中交通标志的位置并识别其类别（如限速、停车和让行标志）.与基于模型的方法相比，基于学习的方法得到了改进并取得了更好的结果.随着深度学习在一般计算机视觉任务中的兴起，卷积神经网络已成为交通标志检测和识别领域的研究热点.</p>
<p>2.2.3路面标线检测与识别:包括检测路面标线的位置并识别其类型（如车道标线、道路标线、信息和人行横道）.</p>
<h2 id="执行决策："><a href="#执行决策：" class="headerlink" title="执行决策："></a>执行决策：</h2><pre><code>目前大部分车辆都采用线控设计，如何将决策通过信号指令控制汽车的油门、制动等相关系统.
</code></pre>
<p>经过环境感知和决策规划之后，就到了执行控制的环节。如何将决策传递给车辆的功能部件，把油门、制动、转向、换挡指令落实，是制动控制的关键，也是标准的动力学原理.</p>
<h2 id="主流实现算法-详"><a href="#主流实现算法-详" class="headerlink" title="主流实现算法(详)"></a>主流实现算法(详)</h2><h3 id="决策矩阵算法"><a href="#决策矩阵算法" class="headerlink" title="决策矩阵算法:"></a>决策矩阵算法:</h3><pre><code>决策矩阵算法能系统分析、识别和评估一组信息集和值之间关系的表现，这些算法主要用户决策.辆的制动或转向是有依据的，它依赖算法对下一个运动的物体的识别、分类、预测的置信水平.决策矩阵算法是由独立训练的各种决策模型组合起来的模型，某种程度上说，这些预测组合在一起构成整体的预测，同时降低决策的错误率.
</code></pre>
<p><strong>AdaBoost:</strong><br>也称自适应增强,其自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器.同时，在每一轮迭代中，加入一个新的弱分类器(60%-80%)，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器(90%以上).<br>其可简化为三个步骤:</p>
<ol>
<li>首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1&#x3D;1&#x2F;N.</li>
<li>然后，训练弱分类器hi。具体训练过程中是：如果某个训练样本点，被弱分类器hi准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去.</li>
<li>最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。<br>换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小.<br><img src="https://pic3.zhimg.com/80/v2-2adde327b2b645738e2b57cb1946591a_1440w.jpg" alt="ada"><br><strong>实质</strong>:以权重对分类器进行筛选,再根据权重比例调配出最终分类器.</li>
</ol>
<h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法:"></a>聚类算法:</h3><pre><code>有时，系统获取的图像不清楚，难以定位和检测对象，分类算法有可能丢失对象.在这种情况下，它们无法对问题分类并将其报告给系统,此时便需要聚类算法对数据进行处理.
</code></pre>
<p><strong>K-means</strong><br>也称k-均值聚类算法,它从数据对象中选择任意k个对象作为初始聚类中心，再根据每个聚类对象的均值（中心对象）计算出每个对象与中心对象的距离，然后根据最小距离重新划分对象.最后重新计算调整后的聚类的均值.<br><img src="https://img-blog.csdnimg.cn/img_convert/92f7530daf6c8851dd2ba44014db30e7.png" alt="k-means"></p>
<h3 id="模式识别算法（分类）"><a href="#模式识别算法（分类）" class="headerlink" title="模式识别算法（分类）"></a>模式识别算法（分类）</h3><pre><code>通过高级驾驶辅助系统（ADAS）中的传感器获得的图像由各种环境数据组成，图像过滤可以用来决定物体分类样例，排除无关的数据点.
</code></pre>
<p><strong>数据简化算法</strong><br>是在对物体进行识别前的一项重要步骤.</p>
<ol>
<li>主成分分析：根据事先指定的信息量（一般是方差最大的是第一主成分），确定主成分分析的层级.</li>
<li>SVD（奇异值分解）：将数据集矩阵（M *N）分解成U（M * M）、E（M * N）、V（N * N）将数据从高维降到低维，减少计算量，保障推荐的效果.</li>
</ol>
<h3 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h3><pre><code>SVM依赖于定义决策边界的决策层概念。决策平面分隔由不同的类成员组成的对象集。
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8995369b329706af6e14b28d5d08365d.png" alt="SVM"><br>SVM是一种二分类模型.它将实例的特征向量映射为空间中的一些点，SVM 的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类.<br>对于任意一个超平面，其两侧数据点都距离它有一个最小距离（垂直距离），这两个最小距离的和就是间隔(margin)，SVM 将会寻找可以区分两个类别并且能使间隔（margin）最大的划分超平面。比较好的划分超平面，样本局部扰动时对它的影响最小、产生的分类结果最鲁棒、对未见示例的泛化能力最强.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Mududu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Mududu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dddleader" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dddleader" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mududu</span>
</div>
  <div class="powered-by">由 Mududu 强力驱动
  </div>

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  















  

  

</body>
</html>
