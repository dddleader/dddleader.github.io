<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="神经网络1 神经网络初探12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989">
<meta property="og:type" content="article">
<meta property="og:title" content="neural-network">
<meta property="og:url" content="http://example.com/2022/10/18/neural-network/index.html">
<meta property="og:site_name" content="Mududu">
<meta property="og:description" content="神经网络1 神经网络初探12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2.png">
<meta property="og:image" content="http://example.com/3.png">
<meta property="og:image" content="http://example.com/4.png">
<meta property="article:published_time" content="2022-10-18T06:30:26.000Z">
<meta property="article:modified_time" content="2022-10-18T13:28:27.653Z">
<meta property="article:author" content="Mududu">
<meta property="article:tag" content="作业">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2.png">

<link rel="canonical" href="http://example.com/2022/10/18/neural-network/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>neural-network | Mududu</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mududu</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/18/neural-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Mududu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mududu">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          neural-network
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-18 14:30:26 / 修改时间：21:28:27" itemprop="dateCreated datePublished" datetime="2022-10-18T14:30:26+08:00">2022-10-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="1-神经网络初探"><a href="#1-神经网络初探" class="headerlink" title="1 神经网络初探"></a>1 神经网络初探</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用numpy生成随机数模拟输入神经网络的数据</span></span><br><span class="line">X = np.random.randint(<span class="number">0</span>, <span class="number">5</span>, size=(<span class="number">500</span>, <span class="number">500</span>))  <span class="comment"># X:数据集的样本特征,共有有效数据500条,每条数据500个特征</span></span><br><span class="line">Y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">1</span>, <span class="number">500</span>))  <span class="comment"># y:数据集的样本标签,500条有效数据，每条对应一个标签，共有500个标签</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="comment"># 本函数为权重参数初始化函数</span></span><br><span class="line"><span class="comment"># 自行决定初始化参数的方式（例如：全零初始化，随机初始化等）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_parameters</span>(<span class="params">in_, out_</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    in_ : 输入层神经元个数</span></span><br><span class="line"><span class="string">    out_: 输出层神经元个数</span></span><br><span class="line"><span class="string">    w: 权重参数weights</span></span><br><span class="line"><span class="string">    b: 偏置参数bias</span></span><br><span class="line"><span class="string">    设计思路:w,b的规模应当满足前向传播计算中矩阵乘法的计算要求</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    w = np.random.randn(out_,in_)*<span class="number">0.01</span></span><br><span class="line">    b = np.random.randn(out_,<span class="number">1</span>)*<span class="number">0.01</span></span><br><span class="line">    <span class="keyword">assert</span> (w.shape == (out_,in_))</span><br><span class="line">    <span class="keyword">assert</span> (b.shape == (out_,<span class="number">1</span>))</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&#x27;w&#x27;</span>: w,</span><br><span class="line">        <span class="string">&#x27;b&#x27;</span>: b</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters  <span class="comment"># 该函数返回初始化好的权重参数字典</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数为前向传播输出预测值</span></span><br><span class="line"><span class="comment"># 本函数实现对字典的更新即可，无需返回值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feed_forward</span>(<span class="params">parameters, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    parameters: 参数字典,即初始化的权重参数</span></span><br><span class="line"><span class="string">    X: 输入值</span></span><br><span class="line"><span class="string">    y_pred: 预测值</span></span><br><span class="line"><span class="string">    设计思路:利用参数字典中的w,b和输入值X，先进性线性变换，再进行激活，完成前向传播。将得到的y_pred添加保存至参数字典中。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w1 = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    z1 = np.dot(w1,X) + b1</span><br><span class="line">    A1 = sigmoid(z1)</span><br><span class="line">    cache = &#123;</span><br><span class="line">        <span class="string">&quot;A1&quot;</span>: A1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (A1,cache)</span><br><span class="line"><span class="comment"># 本函数为反向传播求梯度</span></span><br><span class="line"><span class="comment"># 通过链式法则求出梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">parameters, X, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    parameters: 参数字典</span></span><br><span class="line"><span class="string">    X: 输入数据</span></span><br><span class="line"><span class="string">    y: 输出数据</span></span><br><span class="line"><span class="string">    dw: 权重参数的梯度</span></span><br><span class="line"><span class="string">    db: 偏置参数的梯度</span></span><br><span class="line"><span class="string">    设计思路:通过链式法则，得到w,b对损失函数的偏导数，得出dw,db。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w1 = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    A1 = cache[<span class="string">&quot;A1&quot;</span>]</span><br><span class="line">    dw = (<span class="number">1</span>/<span class="number">1</span>)*np.dot(X,(A1-Y).T)<span class="comment">#(500,1)</span></span><br><span class="line">    db = (<span class="number">1</span>/<span class="number">1</span>)*np.<span class="built_in">sum</span>(A1-Y)</span><br><span class="line">    <span class="comment">#print(db.shape)</span></span><br><span class="line">    <span class="comment">#print(dw.shape,w1.shape,A1.shape,X)</span></span><br><span class="line">    <span class="comment">#assert (dw.shape == w1.shape)</span></span><br><span class="line">    grad = &#123;</span><br><span class="line">        <span class="string">&#x27;dw&#x27;</span>: dw,</span><br><span class="line">        <span class="string">&#x27;db&#x27;</span>: db</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> grad  <span class="comment"># 该函数返回通过反向传播求出的梯度字典</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数为使用梯度下降法更新权重参数</span></span><br><span class="line"><span class="comment"># 所用梯度为经反向传播求得的梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">grad, parameters, lr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    grad: 通过反向传播求出的梯度,保存了当前的dw,db</span></span><br><span class="line"><span class="string">    parameters: 参数字典,保存了当前的w,b</span></span><br><span class="line"><span class="string">    lr: 学习率,影响梯度下降的速度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    w = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    b = parameters[<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    dw = grad[<span class="string">&quot;dw&quot;</span>]</span><br><span class="line">    db = grad[<span class="string">&quot;db&quot;</span>]</span><br><span class="line">    w = w - lr*dw</span><br><span class="line">    b = b - lr*db</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&quot;w&quot;</span>: w,</span><br><span class="line">        <span class="string">&quot;b&quot;</span>: b</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters  <span class="comment"># 该函数返回更新后的参数字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parameters = initial_parameters(<span class="number">500</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line">    A1,cache = feed_forward(parameters,X)</span><br><span class="line">    grad = gradient(parameters,X,Y)</span><br><span class="line">    parameters = update(grad, parameters , <span class="number">0.01</span>)</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第一题较为基础，具体思考过程放至第二题展示</p>
<h2 id="2-搭建属于你的神经网络"><a href="#2-搭建属于你的神经网络" class="headerlink" title="2 搭建属于你的神经网络"></a>2 搭建属于你的神经网络</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol>
<li>代码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = pd.read_csv(<span class="string">&quot;UCI Heart Disease Dataset.csv&quot;</span>)</span><br><span class="line">    y = x[<span class="string">&quot;target&quot;</span>][:,np.newaxis]</span><br><span class="line">    <span class="keyword">del</span> x[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">    x = np.array(x)</span><br><span class="line">    y = np.array(y)</span><br><span class="line">    y.astype(np.int64)</span><br><span class="line">    x = x.T</span><br><span class="line">    y =y.T</span><br><span class="line">    </span><br></pre></td></tr></table></figure></li>
<li>解释：<ol>
<li>转为矩阵更方便各类处理(reshape,切片….)</li>
</ol>
</li>
</ol>
<h3 id="初始化权重参数"><a href="#初始化权重参数" class="headerlink" title="初始化权重参数"></a>初始化权重参数</h3><ol>
<li>代码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X , Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">     X - 输入数据集,维度为（输入的数量，训练/测试的数量）</span></span><br><span class="line"><span class="string">     Y - 标签，维度为（输出的数量，训练/测试数量）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">     n_x - 输入层的数量</span></span><br><span class="line"><span class="string">     n_h - 隐藏层的数量</span></span><br><span class="line"><span class="string">     n_y - 输出层的数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment">#输入层</span></span><br><span class="line">    n_h = <span class="number">30</span> <span class="comment">#，隐藏层个数，硬编码为30</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment">#输出层</span></span><br><span class="line">    <span class="keyword">return</span> (n_x,n_h,n_y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_parameters</span>(<span class="params">in_,hid_,out_</span>):</span><br><span class="line">       np.random.seed(<span class="number">2</span>)</span><br><span class="line">    w1 = np.random.randn(hid_,in_)*np.sqrt(<span class="number">1</span>/<span class="number">13</span>)</span><br><span class="line">    b1 = np.zeros(shape=(hid_,<span class="number">1</span>))</span><br><span class="line">    w2 = np.random.randn(out_,hid_)*np.sqrt(<span class="number">1</span>/hid_)</span><br><span class="line">    b2 = np.zeros(shape=(out_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#适用断言确保数据维度初始化正确</span></span><br><span class="line">    <span class="keyword">assert</span> (w1.shape == (hid_,in_))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (hid_,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (w2.shape == (out_,hid_))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (out_,<span class="number">1</span>))</span><br><span class="line">    parameters =&#123;</span><br><span class="line">        <span class="string">&quot;w1&quot;</span>: w1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;w2&quot;</span>: w2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></li>
<li>为何w随机，b全0：<ol>
<li>w、b均全0初始化缺点：<ol>
<li>若用全0初始化，则会使wx始终为0，使后续的参数迭代与初始数据无关，导致学习失败。</li>
<li><strong>权重的对称性</strong>：如每个隐层的神经元的w都相等，正向传播的时候所有神经元的输出都一致。在反向传播的时候各隐藏层更新也相同。导致多个隐藏神经元的作用如同1个神经元。</li>
</ol>
</li>
<li>w全0，b随机初始化缺点：<ol>
<li>第一次batch的时候由于w为0，使得除了最后一层的w之外的w得不到更新。第二次batch时才可开始更新。</li>
<li>存在更新较慢、梯度消失、梯度爆炸等问题</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h3><ol>
<li>参数：<ol>
<li>定义：就是模型可以根据数据可以自动学习出的变量</li>
<li>本题：各层神经元的权重（w）、偏置（b）</li>
</ol>
</li>
<li>超参数：<ol>
<li>定义：就是用来确定模型的一些参数</li>
<li>本题：学习速率（lr）、迭代次数（iretation_num）、神经网络的层数、每层神经元的个数等</li>
</ol>
</li>
</ol>
<h3 id="激活函数与损失函数"><a href="#激活函数与损失函数" class="headerlink" title="激活函数与损失函数"></a>激活函数与损失函数</h3><ol>
<li><strong>激活函数</strong>：<ol>
<li>作用：<ol>
<li>部分较极端神经网络经过多层叠加会走向极限，影响最终预测。可用激活函数将其限制于一定范围内。</li>
<li>线性回归的表达能力太弱了，对于y&#x3D;ax+b无论叠加多少层最后仍然是线性的，增加网络的深度根本没有意义。激活函数可令神经网络拥有非线性的表达能力。</li>
</ol>
</li>
<li>本题：<ol>
<li>sigmoid函数：二分类问题，最后一层用sigmoid可使最终结果位于（0，1）以表达判断概率。<ol>
<li>缺点：<ol>
<li>梯度消失可能性大</li>
<li>在非最后一层使用会使下一层的输入全为正，使得反向传播更新参数时w全往同一方向更新（捆绑效果）</li>
<li>幂运算较为耗时，增加训练时间</li>
</ol>
</li>
</ol>
</li>
<li>tanh函数<ol>
<li>解决了Sigmoid函数的不是zero-centered输出问题</li>
</ol>
</li>
<li>Relu函数：<ol>
<li>优点<ol>
<li>计算速度快，收敛速度快</li>
<li>正区间中解决梯度消失问题</li>
</ol>
</li>
<li>缺点：<ol>
<li>非零中心</li>
<li>极端情况下某些神经元不会被激活。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">1</span> + np.tanh(<span class="number">0.5</span> * x))</span><br><span class="line"><span class="comment">#防止sigmoid溢出的安全写法</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><strong>损失函数</strong>：<ol>
<li><strong>交叉熵损失函数（二分类）</strong>：<ol>
<li><img src="/2.png"></li>
<li>多分类问题：<br> <img src="/3.png"></li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ol>
<li>工具使用：matplotlib</li>
<li>代码实现：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    plt.figure(<span class="number">1</span>)</span><br><span class="line">    plt.scatter(i,cost)</span><br><span class="line">    <span class="comment">#i为循环次数，cost为计算出的交叉熵损失，此处绘制散点图。</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li>结果展示：<br><img src="/4.png"></li>
</ol>
<h3 id="准确率计算"><a href="#准确率计算" class="headerlink" title="准确率计算"></a>准确率计算</h3><ol>
<li>代码实现：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cache,a2 = feed_foward(x,parameters)</span><br><span class="line">predictions = a2</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;准确率: %d&#x27;</span> % <span class="built_in">float</span>((np.dot(y, predictions.T) + \</span><br><span class="line">                          np.dot(<span class="number">1</span> - y, <span class="number">1</span> - predictions.T)) / <span class="built_in">float</span>(y.size) * <span class="number">100</span>) + <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li>原理解释： accuracy &#x3D; (TP + TN) &#x2F; (TP + FP + TN + FN)</li>
<li>结果：在30个神经元，学习率0.001，循环1500次下准确率仅有54%。在8个神经元，lr&#x3D;0.001，循环100w次准确率有90%以上。本题若想提升准确率还需扩大数据量或提高隐层个数。</li>
</ol>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">warnings.filterwarnings((<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">1</span> + np.tanh(<span class="number">0.5</span> * x))</span><br><span class="line">    <span class="comment">#sigmoid平替</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X , Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">     X - 输入数据集,维度为（输入的数量，训练/测试的数量）</span></span><br><span class="line"><span class="string">     Y - 标签，维度为（输出的数量，训练/测试数量）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">     n_x - 输入层的数量</span></span><br><span class="line"><span class="string">     n_h - 隐藏层的数量</span></span><br><span class="line"><span class="string">     n_y - 输出层的数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment">#输入层</span></span><br><span class="line">    n_h = <span class="number">30</span> <span class="comment">#，隐藏层，硬编码为30</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment">#输出层</span></span><br><span class="line">    <span class="keyword">return</span> (n_x,n_h,n_y)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># 计算每行的最大值</span></span><br><span class="line">    row_max = np.<span class="built_in">max</span>(x)</span><br><span class="line">    <span class="comment"># 每行元素都需要减去对应的最大值，否则求exp(x)会溢出，导致inf情况</span></span><br><span class="line">    x = x - row_max</span><br><span class="line">    <span class="comment"># 计算e的指数次幂</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line">    x_sum = np.<span class="built_in">sum</span>(x_exp)</span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">    <span class="comment">#用于多分类问题，保证p（总）=1.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initial_parameters</span>(<span class="params">in_,hid_,out_</span>):</span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    w1 = np.random.randn(hid_,in_)*np.sqrt(<span class="number">1</span>/<span class="number">13</span>)</span><br><span class="line">    b1 = np.zeros(shape=(hid_,<span class="number">1</span>))</span><br><span class="line">    w2 = np.random.randn(out_,hid_)*np.sqrt(<span class="number">1</span>/hid_)</span><br><span class="line">    b2 = np.zeros(shape=(out_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># assert (w1.shape == (hid_,in_))</span></span><br><span class="line">    <span class="comment"># assert (b1.shape == (hid_,1))</span></span><br><span class="line">    <span class="comment"># assert (w2.shape == (out_,hid_))</span></span><br><span class="line">    <span class="comment"># assert (b2.shape == (hid_,1))</span></span><br><span class="line">    <span class="comment">#可使用断言保证数据维度初始化正确</span></span><br><span class="line">    parameters =&#123;</span><br><span class="line">        <span class="string">&quot;w1&quot;</span>: w1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;w2&quot;</span>: w2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feed_foward</span>(<span class="params">x,parameters</span>):</span><br><span class="line">    w1 = parameters[<span class="string">&quot;w1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    w2 = parameters[<span class="string">&quot;w2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    z1 = np.dot(w1,x)+b1</span><br><span class="line">    a1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(w2,a1)+b2</span><br><span class="line">    a2 = logistic_function(z2)</span><br><span class="line">    <span class="comment">#a2 = sigmoid(z2)</span></span><br><span class="line">    cache = &#123;</span><br><span class="line">        <span class="string">&quot;z1&quot;</span>:z1,</span><br><span class="line">        <span class="string">&quot;a1&quot;</span>:a1,</span><br><span class="line">        <span class="string">&quot;z2&quot;</span>:z2,</span><br><span class="line">        <span class="string">&quot;a2&quot;</span>:a2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (cache,a2)</span><br><span class="line">    <span class="comment">#cache用于储存已计算的结果，加快反向传播速度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prediction</span>(<span class="params">x,parameters</span>):</span><br><span class="line">    cache,a2 = feed_foward(x,parameters)</span><br><span class="line">    predictions = np.<span class="built_in">round</span>(a2)</span><br><span class="line">    <span class="comment">#取整，以0/1表示分类结果</span></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">a2,y</span>):</span><br><span class="line">    m = y.shape[<span class="number">1</span>]</span><br><span class="line">    logprobs = np.multiply(np.log(a2),y)+np.multiply((<span class="number">1</span>-y),np.log(<span class="number">1</span>-a2))</span><br><span class="line">    <span class="comment">#计算交叉熵损失</span></span><br><span class="line">    cost = - np.<span class="built_in">sum</span>(logprobs) / m</span><br><span class="line">    <span class="comment">#求平均值</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">parameters,cache,x,y</span>):</span><br><span class="line">    m = x.shape[<span class="number">1</span>]</span><br><span class="line">    w1 = parameters[<span class="string">&quot;w1&quot;</span>]</span><br><span class="line">    w2 = parameters[<span class="string">&quot;w2&quot;</span>]</span><br><span class="line">    a1 = cache[<span class="string">&quot;a1&quot;</span>]</span><br><span class="line">    a2 = cache[<span class="string">&quot;a2&quot;</span>]</span><br><span class="line">    dz2 = a2-y</span><br><span class="line">    dw2 = (<span class="number">1</span>/m)*np.dot(dz2,a1.T)</span><br><span class="line">    db2 = (<span class="number">1</span>/m)*np.<span class="built_in">sum</span>(dz2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    dz1 = np.multiply(np.dot(w2.T,dz2),<span class="number">1</span>-np.power(a1,<span class="number">2</span>))</span><br><span class="line">    dw1 = (<span class="number">1</span>/m)*np.dot(dz1,x.T)</span><br><span class="line">    db1 = (<span class="number">1</span>/m)*np.<span class="built_in">sum</span>(dz1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#向量化计算反向传播，返回w、b等偏导数的矩阵</span></span><br><span class="line">    grads = &#123;</span><br><span class="line">        <span class="string">&quot;dw2&quot;</span>:dw2,</span><br><span class="line">        <span class="string">&quot;db2&quot;</span>:db2,</span><br><span class="line">        <span class="string">&quot;dw1&quot;</span>:dw1,</span><br><span class="line">        <span class="string">&quot;db1&quot;</span>:db1</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters,grads,lr</span>):</span><br><span class="line">    w1 = parameters[<span class="string">&quot;w1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    w2 = parameters[<span class="string">&quot;w2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    dw1,dw2 = grads[<span class="string">&quot;dw1&quot;</span>],grads[<span class="string">&quot;dw2&quot;</span>]</span><br><span class="line">    db1,db2 = grads[<span class="string">&quot;db1&quot;</span>],grads[<span class="string">&quot;db2&quot;</span>]</span><br><span class="line">    w1 = w1 - lr * dw1</span><br><span class="line">    b1 = b1 - lr * db1</span><br><span class="line">    w2 = w2 - lr * dw2</span><br><span class="line">    b2 = b2 - lr * db2</span><br><span class="line">    <span class="comment">#更新参数</span></span><br><span class="line">    <span class="comment">#学习率过大：跳过最优解，函数难以收敛</span></span><br><span class="line">    <span class="comment">#学习率过小：网络收敛非常缓慢，会增大找到最优值的时间。很可能会进入局部极值点就收敛，没有真正找到的最优解。</span></span><br><span class="line">    parameters =&#123;</span><br><span class="line">        <span class="string">&quot;w1&quot;</span>: w1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;w2&quot;</span>: w2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    x = pd.read_csv(<span class="string">&quot;UCI Heart Disease Dataset.csv&quot;</span>)</span><br><span class="line">    y = x[<span class="string">&quot;target&quot;</span>][:,np.newaxis]</span><br><span class="line">    <span class="keyword">del</span> x[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">    x = np.array(x)</span><br><span class="line">    y = np.array(y)</span><br><span class="line">    y.astype(np.int64)</span><br><span class="line">    x = x.T</span><br><span class="line">    y =y.T</span><br><span class="line">    n_x, n_h, n_y = layer_sizes(x,y)</span><br><span class="line">    parameters = initial_parameters(n_x,n_h,n_y)</span><br><span class="line">    num_iterations =<span class="number">1500</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (num_iterations):</span><br><span class="line">        cache, a2 = feed_foward(x, parameters)</span><br><span class="line">        cost = compute_cost(a2,y)</span><br><span class="line">        grads = gradient(parameters, cache, x, y)</span><br><span class="line">        parameters = update_parameters(parameters,grads,<span class="number">0.001</span>)</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(cost)</span><br><span class="line">        plt.figure(<span class="number">1</span>)</span><br><span class="line">        plt.scatter(i,cost)</span><br><span class="line">    plt.show()</span><br><span class="line">    cache,a2 = feed_foward(x,parameters)</span><br><span class="line">    predictions = a2</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;准确率: %d&#x27;</span> % <span class="built_in">float</span>((np.dot(y, predictions.T) + \</span><br><span class="line">                              np.dot(<span class="number">1</span> - y, <span class="number">1</span> - predictions.T)) / <span class="built_in">float</span>(y.size) * <span class="number">100</span>) + <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Mududu 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Mududu 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BD%9C%E4%B8%9A/" rel="tag"># 作业</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/10/18/random-forest/" rel="prev" title="random-forest">
      <i class="fa fa-chevron-left"></i> random-forest
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/10/18/numpy/" rel="next" title="numpy">
      numpy <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2"><span class="nav-number">1.1.</span> <span class="nav-text">1 神经网络初探</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%90%AD%E5%BB%BA%E5%B1%9E%E4%BA%8E%E4%BD%A0%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.</span> <span class="nav-text">2 搭建属于你的神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.2.1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%E5%8F%82%E6%95%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">初始化权重参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">参数与超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.4.</span> <span class="nav-text">激活函数与损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">1.2.5.</span> <span class="nav-text">可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97"><span class="nav-number">1.2.6.</span> <span class="nav-text">准确率计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%9A"><span class="nav-number">1.2.7.</span> <span class="nav-text">代码实现：</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Mududu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Mududu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dddleader" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dddleader" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mududu</span>
</div>
  <div class="powered-by">由 Mududu 强力驱动
  </div>

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  















  

  

</body>
</html>
